{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 13440166,
          "sourceType": "datasetVersion",
          "datasetId": 8530763
        },
        {
          "sourceId": 13440382,
          "sourceType": "datasetVersion",
          "datasetId": 8530909
        },
        {
          "sourceId": 13444159,
          "sourceType": "datasetVersion",
          "datasetId": 8533600
        },
        {
          "sourceId": 13456723,
          "sourceType": "datasetVersion",
          "datasetId": 8541854
        }
      ],
      "dockerImageVersionId": 31154,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "OCAT model training",
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "source": [
        "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
        "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
        "import kagglehub\n",
        "kagglehub.login()\n"
      ],
      "metadata": {
        "id": "LXrI9OdqB3FM"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "varshitpashikanti_labeled_ocat_path = kagglehub.dataset_download('varshitpashikanti/labeled-ocat')\n",
        "varshitpashikanti_ocat_clips_path = kagglehub.dataset_download('varshitpashikanti/ocat-clips')\n",
        "varshitpashikanti_prelabes_path = kagglehub.dataset_download('varshitpashikanti/prelabes')\n",
        "varshitpashikanti_engineered_features_path = kagglehub.dataset_download('varshitpashikanti/engineered-features')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "s_KYAc6AB3FP"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "import glob\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import math\n",
        "\n",
        "# --- Configuration ---\n",
        "# !!! YOU MUST SET THESE !!!\n",
        "VIDEO_WIDTH = 1920  # e.g., 1920 (for 1080p)\n",
        "VIDEO_HEIGHT = 1080 # e.g., 1080 (for 1080p)\n",
        "\n",
        "# --- Directories ---\n",
        "INPUT_DIR = '/kaggle/input/prelabeling-ocat/part_3'  # Folder containing your .npy files\n",
        "OUTPUT_FILE = '/kaggle/working/draft_labels4.csv'       # The resulting CSV file\n",
        "\n",
        "# --- Rule Thresholds ---\n",
        "# Blinking\n",
        "EAR_THRESHOLD = 0.21 # Threshold for a single frame to be 'closed'\n",
        "\n",
        "# Gaze (0.0 = far left, 1.0 = far right, 0.5 = center)\n",
        "GAZE_THRESHOLD_LOW = 0.35\n",
        "GAZE_THRESHOLD_HIGH = 0.65\n",
        "\n",
        "# --- MediaPipe Landmark Indices ---\n",
        "# These are fixed indices from MediaPipe's 478-landmark model\n",
        "LEFT_EYE_LANDMARKS = [362, 385, 387, 263, 373, 380]\n",
        "RIGHT_EYE_LANDMARKS = [33, 160, 158, 133, 153, 144]\n",
        "\n",
        "LEFT_IRIS_LANDMARKS = [474, 475, 476, 477]\n",
        "RIGHT_IRIS_LANDMARKS = [469, 470, 471, 472]\n",
        "\n",
        "# For 3D Head Pose\n",
        "HEAD_POSE_LANDMARKS = [\n",
        "    33, 263, 1, 61, 291, 199 # R-eye, L-eye, Nose, R-mouth, L-mouth, Chin\n",
        "]\n",
        "\n",
        "# 3D Canonical Face Model Points (from MediaPipe docs)\n",
        "# These correspond to the HEAD_POSE_LANDMARKS indices\n",
        "# https://github.com/google/mediapipe/blob/master/mediapipe/modules/face_geometry/data/canonical_face_model.obj\n",
        "CANONICAL_FACE_MODEL = np.array(\n",
        "    [\n",
        "        [ 0.033501,  0.068864, -0.052668], # 33  - Right Eye\n",
        "        [-0.033501,  0.068864, -0.052668], # 263 - Left Eye\n",
        "        [ 0.000000,  0.000000, -0.000000], # 1   - Nose Tip\n",
        "        [ 0.046330, -0.045969, -0.032640], # 61  - Right Mouth Corner\n",
        "        [-0.046330, -0.045969, -0.032640], # 291 - Left Mouth Corner\n",
        "        [ 0.000000, -0.104473, -0.009363]  # 199 - Chin\n",
        "    ], dtype=np.float32\n",
        ") * 100 # Scale up for better solvePnP stability\n",
        "\n",
        "\n",
        "# --- Helper Functions ---\n",
        "\n",
        "def get_2d_points(landmarks_3d, width, height):\n",
        "    \"\"\"Converts normalized 3D landmarks to 2D pixel coordinates.\"\"\"\n",
        "    # landmarks_3d shape is (478, 3) where [:, 0] is x, [:, 1] is y\n",
        "    # We only need x and y for 2D\n",
        "    # We multiply normalized coords by video dimensions\n",
        "    return landmarks_3d[:, :2] * [width, height]\n",
        "\n",
        "def get_ear(eye_points):\n",
        "    \"\"\"Calculates the Eye Aspect Ratio (EAR) from 6 eye landmarks.\"\"\"\n",
        "    # eye_points shape is (6, 2)\n",
        "    #      p2 -- p3\n",
        "    # p1 /        \\ p4\n",
        "    #    \\        /\n",
        "    #      p6 -- p5\n",
        "\n",
        "    try:\n",
        "        # Vertical distances\n",
        "        v1 = np.linalg.norm(eye_points[1] - eye_points[5])\n",
        "        v2 = np.linalg.norm(eye_points[2] - eye_points[4])\n",
        "        # Horizontal distance\n",
        "        h = np.linalg.norm(eye_points[0] - eye_points[3])\n",
        "\n",
        "        if h == 0:\n",
        "            return 0.0\n",
        "\n",
        "        ear = (v1 + v2) / (2.0 * h)\n",
        "        return ear\n",
        "    except:\n",
        "        return 0.0\n",
        "\n",
        "def get_gaze_ratio(eye_points, iris_center):\n",
        "    \"\"\"\n",
        "    Calculates horizontal gaze ratio.\n",
        "    < 0.35 = looking left\n",
        "    > 0.65 = looking right\n",
        "    ~ 0.5 = centered\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Get horizontal center of the eye\n",
        "        eye_left_x = eye_points[0][0]\n",
        "        eye_right_x = eye_points[3][0]\n",
        "        eye_width = eye_right_x - eye_left_x\n",
        "\n",
        "        if eye_width == 0:\n",
        "            return 0.5 # Assume center if eye not detected\n",
        "\n",
        "        gaze_ratio = (iris_center[0] - eye_left_x) / eye_width\n",
        "        return np.clip(gaze_ratio, 0.0, 1.0)\n",
        "    except:\n",
        "        return 0.5 # Default to center on error\n",
        "\n",
        "def get_head_pose(landmarks_3d, width, height):\n",
        "    \"\"\"\n",
        "    Estimates head pose (yaw, pitch, roll) using cv2.solvePnP.\n",
        "    Returns yaw in degrees.\n",
        "    \"\"\"\n",
        "\n",
        "    # Get 2D pixel coordinates for the 6 key points\n",
        "    image_points = landmarks_3d[HEAD_POSE_LANDMARKS, :2] * [width, height]\n",
        "\n",
        "    # Get the 3D model points\n",
        "    model_points = CANONICAL_FACE_MODEL\n",
        "\n",
        "    # Camera matrix (assuming simple pinhole camera)\n",
        "    focal_length = width\n",
        "    center = (width / 2, height / 2)\n",
        "    camera_matrix = np.array(\n",
        "        [[focal_length, 0, center[0]],\n",
        "         [0, focal_length, center[1]],\n",
        "         [0, 0, 1]], dtype=\"double\"\n",
        "    )\n",
        "\n",
        "    # Distortion coefficients (assuming no distortion)\n",
        "    dist_coeffs = np.zeros((4, 1))\n",
        "\n",
        "    try:\n",
        "        # Solve for rotation and translation\n",
        "        (success, rotation_vector, translation_vector) = cv2.solvePnP(\n",
        "            model_points,\n",
        "            image_points,\n",
        "            camera_matrix,\n",
        "            dist_coeffs,\n",
        "            flags=cv2.SOLVEPNP_ITERATIVE\n",
        "        )\n",
        "\n",
        "        # Convert rotation vector to rotation matrix\n",
        "        rotation_matrix, _ = cv2.Rodrigues(rotation_vector)\n",
        "\n",
        "        # Get Euler angles (yaw, pitch, roll)\n",
        "        # See: https://www.learnopencv.com/rotation-matrix-to-euler-angles/\n",
        "        sy = math.sqrt(rotation_matrix[0, 0] * rotation_matrix[0, 0] + rotation_matrix[1, 0] * rotation_matrix[1, 0])\n",
        "        singular = sy < 1e-6\n",
        "\n",
        "        if not singular:\n",
        "            x = math.atan2(rotation_matrix[2, 1], rotation_matrix[2, 2]) # Roll\n",
        "            y = math.atan2(-rotation_matrix[2, 0], sy)                   # Pitch\n",
        "            z = math.atan2(rotation_matrix[1, 0], rotation_matrix[0, 0]) # Yaw\n",
        "        else:\n",
        "            x = math.atan2(-rotation_matrix[1, 2], rotation_matrix[1, 1])\n",
        "            y = math.atan2(-rotation_matrix[2, 0], sy)\n",
        "            z = 0\n",
        "\n",
        "        # Convert yaw (z) to degrees\n",
        "        # Positive values mean turning to the left, negative to the right.\n",
        "        # We'll flip the sign so positive = right, negative = left, like in the rules.\n",
        "        yaw_degrees = -z * (180.0 / math.pi)\n",
        "        return yaw_degrees\n",
        "\n",
        "    except Exception as e:\n",
        "        # print(f\"Error in solvePnP: {e}\")\n",
        "        return 0.0 # Default to 0 if calculation fails\n",
        "\n",
        "# --- Main Processing Loop ---\n",
        "\n",
        "# Find all .npy files\n",
        "npy_files = glob.glob(os.path.join(INPUT_DIR, '*.npy'))\n",
        "print(f\"Found {len(npy_files)} feature files. Processing...\")\n",
        "\n",
        "clip_results = []\n",
        "\n",
        "for file_path in tqdm(npy_files, desc=\"Processing Clips\"):\n",
        "    try:\n",
        "        clip_data = np.load(file_path) # Shape (num_frames, 1434)\n",
        "        num_frames = clip_data.shape[0]\n",
        "\n",
        "        if num_frames == 0:\n",
        "            # print(f\"Skipping empty file: {file_path}\")\n",
        "            continue\n",
        "\n",
        "        blink_frames = 0\n",
        "        gaze_off_center_frames = 0\n",
        "        yaw_values = []\n",
        "        valid_frames = 0\n",
        "\n",
        "        for frame_idx in range(num_frames):\n",
        "            frame_landmarks_flat = clip_data[frame_idx]\n",
        "\n",
        "            # Check for empty frames (where no face was detected)\n",
        "            if np.all(frame_landmarks_flat == 0):\n",
        "                continue\n",
        "\n",
        "            valid_frames += 1\n",
        "\n",
        "            # Reshape to (478, 3) to access x, y, z\n",
        "            landmarks_3d = frame_landmarks_flat.reshape((478, 3))\n",
        "\n",
        "            # Convert to 2D pixel coordinates for EAR and Gaze\n",
        "            landmarks_2d = get_2d_points(landmarks_3d, VIDEO_WIDTH, VIDEO_HEIGHT)\n",
        "\n",
        "            # 1. Calculate Eye Blink\n",
        "            left_ear = get_ear(landmarks_2d[LEFT_EYE_LANDMARKS])\n",
        "            right_ear = get_ear(landmarks_2d[RIGHT_EYE_LANDMARKS])\n",
        "            avg_ear = (left_ear + right_ear) / 2.0\n",
        "\n",
        "            if avg_ear < EAR_THRESHOLD:\n",
        "                blink_frames += 1\n",
        "\n",
        "            # 2. Calculate Gaze\n",
        "            left_iris_center = landmarks_2d[LEFT_IRIS_LANDMARKS].mean(axis=0)\n",
        "            right_iris_center = landmarks_2d[RIGHT_IRIS_LANDMARKS].mean(axis=0)\n",
        "\n",
        "            left_gaze = get_gaze_ratio(landmarks_2d[LEFT_EYE_LANDMARKS], left_iris_center)\n",
        "            right_gaze = get_gaze_ratio(landmarks_2d[RIGHT_EYE_LANDMARKS], right_iris_center)\n",
        "            avg_gaze = (left_gaze + right_gaze) / 2.0\n",
        "\n",
        "            if not (GAZE_THRESHOLD_LOW < avg_gaze < GAZE_THRESHOLD_HIGH):\n",
        "                gaze_off_center_frames += 1\n",
        "\n",
        "            # 3. Calculate Head Yaw\n",
        "            # We use the raw 3D landmarks for this\n",
        "            yaw = get_head_pose(landmarks_3d, VIDEO_WIDTH, VIDEO_HEIGHT)\n",
        "            yaw_values.append(yaw)\n",
        "\n",
        "        if valid_frames == 0:\n",
        "            # print(f\"Skipping clip with no face detections: {file_path}\")\n",
        "            continue\n",
        "\n",
        "        # --- Calculate Clip-Level Statistics ---\n",
        "        percent_eyes_closed = (blink_frames / valid_frames) * 100\n",
        "        gaze_off_center_time = (gaze_off_center_frames / valid_frames) * 100\n",
        "        avg_head_yaw = np.mean(yaw_values) if yaw_values else 0.0\n",
        "\n",
        "        # --- Apply Rules for Draft Label ---\n",
        "        draft_label = \"PRE_Neutral\" # Default\n",
        "\n",
        "        # Rule 1: Distracted\n",
        "        if (avg_head_yaw > 25 or avg_head_yaw < -25) or (percent_eyes_closed > 40):\n",
        "            draft_label = \"PRE_Distracted\"\n",
        "\n",
        "        # Rule 2: Attentive (overrides Distracted if also true, though unlikely)\n",
        "        # Note: Added abs() to yaw as < 10° implies looking straight.\n",
        "        elif (abs(avg_head_yaw) < 10) and (gaze_off_center_time < 20):\n",
        "            draft_label = \"PRE_Attentive\"\n",
        "\n",
        "        # --- Store Result ---\n",
        "        clip_name = os.path.splitext(os.path.basename(file_path))[0]\n",
        "        clip_results.append({\n",
        "            \"clip_name\": clip_name,\n",
        "            \"draft_label\": draft_label,\n",
        "            \"avg_head_yaw\": round(avg_head_yaw, 2),\n",
        "            \"percent_eyes_closed\": round(percent_eyes_closed, 2),\n",
        "            \"gaze_off_center_time\": round(gaze_off_center_time, 2),\n",
        "            \"valid_frames\": valid_frames,\n",
        "            \"total_frames\": num_frames\n",
        "        })\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to process {file_path}. Error: {e}\")\n",
        "\n",
        "# --- Save Final CSV ---\n",
        "df = pd.DataFrame(clip_results)\n",
        "df.to_csv(OUTPUT_FILE, index=False)\n",
        "\n",
        "print(f\"\\nProcessing complete. Saved {len(df)} clips to {OUTPUT_FILE}\")"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-25T17:44:54.598968Z",
          "iopub.execute_input": "2025-10-25T17:44:54.599538Z",
          "iopub.status.idle": "2025-10-25T17:44:54.62611Z",
          "shell.execute_reply.started": "2025-10-25T17:44:54.599513Z",
          "shell.execute_reply": "2025-10-25T17:44:54.625241Z"
        },
        "id": "rxuT6izwB3FQ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import glob\n",
        "\n",
        "files = glob.glob(\"/kaggle/working/*.csv\")\n",
        "merged_df = pd.concat([pd.read_csv(f) for f in files], ignore_index=True)\n",
        "merged_df.to_csv(\"merged_output.csv\", index=False)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-25T17:44:54.627172Z",
          "iopub.execute_input": "2025-10-25T17:44:54.627417Z",
          "iopub.status.idle": "2025-10-25T17:44:54.889388Z",
          "shell.execute_reply.started": "2025-10-25T17:44:54.627391Z",
          "shell.execute_reply": "2025-10-25T17:44:54.887984Z"
        },
        "id": "JqUmvY3pB3FS"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read the merged CSV\n",
        "df = pd.read_csv(\"merged_output.csv\")\n",
        "\n",
        "# Sort rows by 'clip_name'\n",
        "df_sorted = df.sort_values(by=\"clip_name\")\n",
        "\n",
        "# Save the sorted CSV\n",
        "df_sorted.to_csv(\"grouped_output.csv\", index=False)\n",
        "\n",
        "print(\"✅ Rows grouped (sorted) by 'clip_name' and saved as 'grouped_output.csv'\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-25T17:44:54.889787Z",
          "iopub.status.idle": "2025-10-25T17:44:54.890053Z",
          "shell.execute_reply.started": "2025-10-25T17:44:54.889936Z",
          "shell.execute_reply": "2025-10-25T17:44:54.889949Z"
        },
        "id": "vgiCvdMkB3FS"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "import glob\n",
        "from tqdm import tqdm\n",
        "import math\n",
        "\n",
        "# --- Configuration ---\n",
        "# !!! YOU MUST SET THESE !!!\n",
        "VIDEO_WIDTH = 1920  # e.g., 1920 (for 1080p)\n",
        "VIDEO_HEIGHT = 1080 # e.g., 1080 (for 1080p)\n",
        "\n",
        "# --- Directories ---\n",
        "RAW_FEATURES_DIR = '/kaggle/input/prelabeling-ocat/part_4'   # Folder with large (90, 1434) .npy files\n",
        "ENGINEERED_FEATURES_DIR = '/kaggle/working/engineered_features4' # Where to save small (90, 10) .npy files\n",
        "\n",
        "# --- MediaPipe Landmark Indices ---\n",
        "# We list only the indices we need to extract\n",
        "LEFT_EYE_LANDMARKS = [362, 385, 387, 263, 373, 380]\n",
        "RIGHT_EYE_LANDMARKS = [33, 160, 158, 133, 153, 144]\n",
        "\n",
        "LEFT_IRIS_LANDMARKS = [474, 475, 476, 477]\n",
        "RIGHT_IRIS_LANDMARKS = [469, 470, 471, 472]\n",
        "\n",
        "MOUTH_LANDMARKS = [61, 291, 13, 14] # R-Corner, L-Corner, Upper-Lip, Lower-Lip\n",
        "\n",
        "HEAD_POSE_LANDMARKS = [\n",
        "    33, 263, 1, 61, 291, 199 # R-eye, L-eye, Nose, R-mouth, L-mouth, Chin\n",
        "]\n",
        "\n",
        "# 3D Canonical Face Model Points\n",
        "CANONICAL_FACE_MODEL = np.array(\n",
        "    [\n",
        "        [ 0.033501,  0.068864, -0.052668], # 33\n",
        "        [-0.033501,  0.068864, -0.052668], # 263\n",
        "        [ 0.000000,  0.000000, -0.000000], # 1\n",
        "        [ 0.046330, -0.045969, -0.032640], # 61\n",
        "        [-0.046330, -0.045969, -0.032640], # 291\n",
        "        [ 0.000000, -0.104473, -0.009363]  # 199\n",
        "    ], dtype=np.float32\n",
        ") * 100\n",
        "\n",
        "NUM_ENGINEERED_FEATURES = 10 # Our 10 selected features\n",
        "\n",
        "# --- Helper Functions ---\n",
        "\n",
        "def get_2d_points(landmarks_3d, width, height):\n",
        "    \"\"\"Converts normalized 3D landmarks to 2D pixel coordinates.\"\"\"\n",
        "    return landmarks_3d[:, :2] * [width, height]\n",
        "\n",
        "def get_ear(eye_points):\n",
        "    \"\"\"Calculates the Eye Aspect Ratio (EAR) from 6 eye landmarks.\"\"\"\n",
        "    try:\n",
        "        v1 = np.linalg.norm(eye_points[1] - eye_points[5])\n",
        "        v2 = np.linalg.norm(eye_points[2] - eye_points[4])\n",
        "        h = np.linalg.norm(eye_points[0] - eye_points[3])\n",
        "        if h == 0: return 0.0\n",
        "        return (v1 + v2) / (2.0 * h)\n",
        "    except:\n",
        "        return 0.0\n",
        "\n",
        "def get_mar(mouth_points):\n",
        "    \"\"\"Calculates Mouth Aspect Ratio (MAR) from 4 landmarks.\"\"\"\n",
        "    try:\n",
        "        horizontal_dist = np.linalg.norm(mouth_points[0] - mouth_points[1])\n",
        "        vertical_dist = np.linalg.norm(mouth_points[2] - mouth_points[3])\n",
        "        if horizontal_dist == 0: return 0.0\n",
        "        return vertical_dist / horizontal_dist\n",
        "    except:\n",
        "        return 0.0\n",
        "\n",
        "def get_relative_iris_pos(eye_points, iris_center):\n",
        "    \"\"\"Calculates normalized iris position relative to the eye center.\"\"\"\n",
        "    try:\n",
        "        eye_center = eye_points.mean(axis=0)\n",
        "        relative_pos = iris_center - eye_center\n",
        "        eye_width = np.linalg.norm(eye_points[0] - eye_points[3])\n",
        "        if eye_width == 0: return 0.0, 0.0\n",
        "        return relative_pos[0] / eye_width, relative_pos[1] / eye_width\n",
        "    except:\n",
        "        return 0.0, 0.0\n",
        "\n",
        "def get_head_pose(landmarks_3d, width, height):\n",
        "    \"\"\"Estimates head pose (yaw, pitch, roll).\"\"\"\n",
        "    image_points = landmarks_3d[HEAD_POSE_LANDMARKS, :2] * [width, height]\n",
        "    model_points = CANONICAL_FACE_MODEL\n",
        "\n",
        "    camera_matrix = np.array(\n",
        "        [[width, 0, width / 2],\n",
        "         [0, width, height / 2],\n",
        "         [0, 0, 1]], dtype=\"double\"\n",
        "    )\n",
        "    dist_coeffs = np.zeros((4, 1))\n",
        "\n",
        "    try:\n",
        "        (success, rotation_vector, t_vec) = cv2.solvePnP(\n",
        "            model_points, image_points, camera_matrix, dist_coeffs, flags=cv2.SOLVEPNP_ITERATIVE\n",
        "        )\n",
        "\n",
        "        rotation_matrix, _ = cv2.Rodrigues(rotation_vector)\n",
        "\n",
        "        sy = math.sqrt(rotation_matrix[0, 0] * rotation_matrix[0, 0] + rotation_matrix[1, 0] * rotation_matrix[1, 0])\n",
        "        singular = sy < 1e-6\n",
        "\n",
        "        if not singular:\n",
        "            roll = math.atan2(rotation_matrix[2, 1], rotation_matrix[2, 2])\n",
        "            pitch = math.atan2(-rotation_matrix[2, 0], sy)\n",
        "            yaw = math.atan2(rotation_matrix[1, 0], rotation_matrix[0, 0])\n",
        "        else:\n",
        "            roll = math.atan2(-rotation_matrix[1, 2], rotation_matrix[1, 1])\n",
        "            pitch = math.atan2(-rotation_matrix[2, 0], sy)\n",
        "            yaw = 0\n",
        "\n",
        "        # Convert to degrees and use intuitive signs\n",
        "        yaw_deg = -yaw * (180.0 / math.pi)   # Positive = turns right\n",
        "        pitch_deg = pitch * (180.0 / math.pi) # Positive = looks up\n",
        "        roll_deg = roll * (180.0 / math.pi)   # Positive = rolls right\n",
        "        return yaw_deg, pitch_deg, roll_deg\n",
        "\n",
        "    except Exception as e:\n",
        "        return 0.0, 0.0, 0.0 # Default to 0\n",
        "\n",
        "# --- Main Processing Loop ---\n",
        "\n",
        "os.makedirs(ENGINEERED_FEATURES_DIR, exist_ok=True)\n",
        "\n",
        "raw_npy_files = glob.glob(os.path.join(RAW_FEATURES_DIR, '*.npy'))\n",
        "print(f\"Found {len(raw_npy_files)} raw feature files. Starting engineering...\")\n",
        "\n",
        "for file_path in tqdm(raw_npy_files, desc=\"Engineering features\"):\n",
        "    try:\n",
        "        raw_clip_data = np.load(file_path) # Shape (num_frames, 1434)\n",
        "        num_frames = raw_clip_data.shape[0]\n",
        "\n",
        "        if num_frames == 0:\n",
        "            continue\n",
        "\n",
        "        engineered_feature_sequence = [] # List to hold the 10 features per frame\n",
        "\n",
        "        for frame_idx in range(num_frames):\n",
        "            frame_landmarks_flat = raw_clip_data[frame_idx]\n",
        "\n",
        "            # If no face was detected, append zeros for all 10 features\n",
        "            if np.all(frame_landmarks_flat == 0):\n",
        "                engineered_feature_sequence.append(np.zeros(NUM_ENGINEERED_FEATURES))\n",
        "                continue\n",
        "\n",
        "            # Reshape to (478, 3) to access x, y, z\n",
        "            landmarks_3d = frame_landmarks_flat.reshape((478, 3))\n",
        "\n",
        "            # Convert to 2D pixel coordinates for 2D calculations\n",
        "            landmarks_2d = get_2d_points(landmarks_3d, VIDEO_WIDTH, VIDEO_HEIGHT)\n",
        "\n",
        "            # 1. & 2. Eye Aspect Ratios\n",
        "            left_ear = get_ear(landmarks_2d[LEFT_EYE_LANDMARKS])\n",
        "            right_ear = get_ear(landmarks_2d[RIGHT_EYE_LANDMARKS])\n",
        "\n",
        "            # 3, 4, & 5. Head Pose\n",
        "            yaw, pitch, roll = get_head_pose(landmarks_3d, VIDEO_WIDTH, VIDEO_HEIGHT)\n",
        "\n",
        "            # 6. & 7. Left Iris Position\n",
        "            left_iris_center = landmarks_2d[LEFT_IRIS_LANDMARKS].mean(axis=0)\n",
        "            rel_left_iris_x, rel_left_iris_y = get_relative_iris_pos(\n",
        "                landmarks_2d[LEFT_EYE_LANDMARKS], left_iris_center\n",
        "            )\n",
        "\n",
        "            # 8. & 9. Right Iris Position\n",
        "            right_iris_center = landmarks_2d[RIGHT_IRIS_LANDMARKS].mean(axis=0)\n",
        "            rel_right_iris_x, rel_right_iris_y = get_relative_iris_pos(\n",
        "                landmarks_2d[RIGHT_EYE_LANDMARKS], right_iris_center\n",
        "            )\n",
        "\n",
        "            # 10. Mouth Aspect Ratio\n",
        "            mar = get_mar(landmarks_2d[MOUTH_LANDMARKS])\n",
        "\n",
        "            # Append all 10 features for this frame\n",
        "            frame_features = [\n",
        "                left_ear, right_ear,\n",
        "                yaw, pitch, roll,\n",
        "                rel_left_iris_x, rel_left_iris_y,\n",
        "                rel_right_iris_x, rel_right_iris_y,\n",
        "                mar\n",
        "            ]\n",
        "            engineered_feature_sequence.append(frame_features)\n",
        "\n",
        "        # Save the new, lightweight (num_frames, 10) array\n",
        "        engineered_array = np.array(engineered_feature_sequence, dtype=np.float32)\n",
        "\n",
        "        base_name = os.path.splitext(os.path.basename(file_path))[0]\n",
        "        output_path = os.path.join(ENGINEERED_FEATURES_DIR, f\"{base_name}.npy\")\n",
        "        np.save(output_path, engineered_array)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to process {file_path}. Error: {e}\")\n",
        "\n",
        "print(f\"\\nFeature engineering complete. All lightweight .npy files saved to {ENGINEERED_FEATURES_DIR}\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-25T17:44:54.891191Z",
          "iopub.status.idle": "2025-10-25T17:44:54.891519Z",
          "shell.execute_reply.started": "2025-10-25T17:44:54.891326Z",
          "shell.execute_reply": "2025-10-25T17:44:54.891342Z"
        },
        "id": "drifJK5IB3FT"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "# Folder to zip\n",
        "folder_path = '/kaggle/working/path/to/engineered_features_merged'\n",
        "\n",
        "# Output zip file path (without .zip extension)\n",
        "output_zip = '/kaggle/working/engineered_features_merged'\n",
        "\n",
        "# Create zip archive\n",
        "shutil.make_archive(output_zip, 'zip', folder_path)\n",
        "\n",
        "print(\"✅ Folder zipped successfully!\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-25T17:44:54.892256Z",
          "iopub.status.idle": "2025-10-25T17:44:54.892495Z",
          "shell.execute_reply.started": "2025-10-25T17:44:54.892357Z",
          "shell.execute_reply": "2025-10-25T17:44:54.89239Z"
        },
        "id": "GkD5WreZB3FU"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Extract 15 features**. (10 head features and 5 hand features)"
      ],
      "metadata": {
        "id": "_y_NR9KKB3FU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "import glob\n",
        "from tqdm import tqdm\n",
        "import math\n",
        "\n",
        "# --- Configuration ---\n",
        "# !!! YOU MUST SET THESE !!!\n",
        "VIDEO_WIDTH = 1920  # e.g., 1920 (for 1080p)\n",
        "VIDEO_HEIGHT = 1080 # e.g., 1080 (for 1080p)\n",
        "\n",
        "# --- Directories ---\n",
        "RAW_HOLISTIC_DIR = '/kaggle/input/prelabeling-ocat/part_1'   # Folder with (90, 1659) files\n",
        "ENGINEERED_FEATURES_DIR = '/kaggle/working/engineered_features2' # Where to save (90, 16) files\n",
        "\n",
        "# --- MediaPipe Landmark Indices (HOLISTIC MODEL) ---\n",
        "# These indices are based on a file structure from mp.solutions.holistic\n",
        "# We will assume a flat array structure like:\n",
        "# [ 478*3 face, 33*3 pose, 21*3 left_hand, 21*3 right_hand ]\n",
        "# Total = 1434 (face) + 99 (pose) + 63 (left) + 63 (right) = 1659 features\n",
        "\n",
        "# --- Face (Indices 0 - 1433) ---\n",
        "FACE_START_IDX = 0\n",
        "FACE_LANDMARKS_COUNT = 478\n",
        "LEFT_EYE_LANDMARKS = [362, 385, 387, 263, 373, 380]\n",
        "RIGHT_EYE_LANDMARKS = [33, 160, 158, 133, 153, 144]\n",
        "LEFT_IRIS_LANDMARKS = [474, 475, 476, 477]\n",
        "RIGHT_IRIS_LANDMARKS = [469, 470, 471, 472]\n",
        "MOUTH_LANDMARKS = [61, 291, 13, 14]\n",
        "HEAD_POSE_LANDMARKS = [33, 263, 1, 61, 291, 199]\n",
        "NOSE_TIP_LANDMARK = 1\n",
        "\n",
        "# --- Pose (Indices 1434 - 1532) ---\n",
        "POSE_START_IDX = 1434\n",
        "POSE_LANDMARKS_COUNT = 33\n",
        "LEFT_WRIST_POSE = 15 # Index within pose landmarks\n",
        "RIGHT_WRIST_POSE = 16 # Index within pose landmarks\n",
        "\n",
        "# --- Left Hand (Indices 1533 - 1595) ---\n",
        "LEFT_HAND_START_IDX = 1533\n",
        "LEFT_HAND_LANDMARKS_COUNT = 21\n",
        "\n",
        "# --- Right Hand (Indices 1596 - 1658) ---\n",
        "RIGHT_HAND_START_IDX = 1596\n",
        "RIGHT_HAND_LANDMARKS_COUNT = 21\n",
        "\n",
        "NUM_ENGINEERED_FEATURES = 16 # Our 10 face + 6 hand features\n",
        "\n",
        "# 3D Canonical Face Model Points\n",
        "CANONICAL_FACE_MODEL = np.array(\n",
        "    [\n",
        "        [ 0.033501,  0.068864, -0.052668], # 33\n",
        "        [-0.033501,  0.068864, -0.052668], # 263\n",
        "        [ 0.000000,  0.000000, -0.000000], # 1\n",
        "        [ 0.046330, -0.045969, -0.032640], # 61\n",
        "        [-0.046330, -0.045969, -0.032640], # 291\n",
        "        [ 0.000000, -0.104473, -0.009363]  # 199\n",
        "    ], dtype=np.float32\n",
        ") * 100\n",
        "\n",
        "# --- Helper Functions (Same as before) ---\n",
        "\n",
        "def get_2d_points(landmarks_3d, width, height):\n",
        "    return landmarks_3d[:, :2] * [width, height]\n",
        "\n",
        "def get_ear(eye_points):\n",
        "    try:\n",
        "        v1 = np.linalg.norm(eye_points[1] - eye_points[5])\n",
        "        v2 = np.linalg.norm(eye_points[2] - eye_points[4])\n",
        "        h = np.linalg.norm(eye_points[0] - eye_points[3])\n",
        "        if h == 0: return 0.0\n",
        "        return (v1 + v2) / (2.0 * h)\n",
        "    except: return 0.0\n",
        "\n",
        "def get_mar(mouth_points):\n",
        "    try:\n",
        "        horizontal_dist = np.linalg.norm(mouth_points[0] - mouth_points[1])\n",
        "        vertical_dist = np.linalg.norm(mouth_points[2] - mouth_points[3])\n",
        "        if horizontal_dist == 0: return 0.0\n",
        "        return vertical_dist / horizontal_dist\n",
        "    except: return 0.0\n",
        "\n",
        "def get_relative_iris_pos(eye_points, iris_center):\n",
        "    try:\n",
        "        eye_center = eye_points.mean(axis=0)\n",
        "        relative_pos = iris_center - eye_center\n",
        "        eye_width = np.linalg.norm(eye_points[0] - eye_points[3])\n",
        "        if eye_width == 0: return 0.0, 0.0\n",
        "        return relative_pos[0] / eye_width, relative_pos[1] / eye_width\n",
        "    except: return 0.0, 0.0\n",
        "\n",
        "def get_head_pose(landmarks_3d, width, height):\n",
        "    image_points = landmarks_3d[HEAD_POSE_LANDMARKS, :2] * [width, height]\n",
        "    model_points = CANONICAL_FACE_MODEL\n",
        "\n",
        "    camera_matrix = np.array(\n",
        "        [[width, 0, width / 2],\n",
        "         [0, width, height / 2],\n",
        "         [0, 0, 1]], dtype=\"double\"\n",
        "    )\n",
        "    dist_coeffs = np.zeros((4, 1))\n",
        "\n",
        "    try:\n",
        "        (success, rotation_vector, t_vec) = cv2.solvePnP(\n",
        "            model_points, image_points, camera_matrix, dist_coeffs, flags=cv2.SOLVEPNP_ITERATIVE\n",
        "        )\n",
        "        rotation_matrix, _ = cv2.Rodrigues(rotation_vector)\n",
        "        sy = math.sqrt(rotation_matrix[0, 0]**2 + rotation_matrix[1, 0]**2)\n",
        "        singular = sy < 1e-6\n",
        "        if not singular:\n",
        "            roll = math.atan2(rotation_matrix[2, 1], rotation_matrix[2, 2])\n",
        "            pitch = math.atan2(-rotation_matrix[2, 0], sy)\n",
        "            yaw = math.atan2(rotation_matrix[1, 0], rotation_matrix[0, 0])\n",
        "        else:\n",
        "            roll = math.atan2(-rotation_matrix[1, 2], rotation_matrix[1, 1])\n",
        "            pitch = math.atan2(-rotation_matrix[2, 0], sy)\n",
        "            yaw = 0\n",
        "        yaw_deg = -yaw * (180.0 / math.pi)\n",
        "        pitch_deg = pitch * (180.0 / math.pi)\n",
        "        roll_deg = roll * (180.0 / math.pi)\n",
        "        return yaw_deg, pitch_deg, roll_deg\n",
        "    except: return 0.0, 0.0, 0.0\n",
        "\n",
        "def get_dist(p1, p2):\n",
        "    \"\"\"Calculate Euclidean distance between two 3D points.\"\"\"\n",
        "    return np.linalg.norm(p1 - p2)\n",
        "\n",
        "# --- Main Processing Loop ---\n",
        "\n",
        "os.makedirs(ENGINEERED_FEATURES_DIR, exist_ok=True)\n",
        "\n",
        "raw_npy_files = glob.glob(os.path.join(RAW_HOLISTIC_DIR, '*.npy'))\n",
        "print(f\"Found {len(raw_npy_files)} raw HOLISTIC feature files. Starting engineering...\")\n",
        "\n",
        "for file_path in tqdm(raw_npy_files, desc=\"Engineering features\"):\n",
        "    try:\n",
        "        raw_clip_data = np.load(file_path) # Shape (num_frames, 1659)\n",
        "        num_frames = raw_clip_data.shape[0]\n",
        "\n",
        "        if num_frames == 0:\n",
        "            continue\n",
        "\n",
        "        engineered_feature_sequence = []\n",
        "\n",
        "        for frame_idx in range(num_frames):\n",
        "            frame_landmarks_flat = raw_clip_data[frame_idx]\n",
        "\n",
        "            # If no data, append zeros\n",
        "            if np.all(frame_landmarks_flat == 0):\n",
        "                engineered_feature_sequence.append(np.zeros(NUM_ENGINEERED_FEATURES))\n",
        "                continue\n",
        "\n",
        "            # --- Extract Landmark Groups ---\n",
        "            face_flat = frame_landmarks_flat[FACE_START_IDX : POSE_START_IDX]\n",
        "            pose_flat = frame_landmarks_flat[POSE_START_IDX : LEFT_HAND_START_IDX]\n",
        "            # left_hand_flat = frame_landmarks_flat[LEFT_HAND_START_IDX : RIGHT_HAND_START_IDX]\n",
        "            # right_hand_flat = frame_landmarks_flat[RIGHT_HAND_START_IDX:]\n",
        "\n",
        "            # --- Reshape ---\n",
        "            landmarks_3d_face = face_flat.reshape((FACE_LANDMARKS_COUNT, 3))\n",
        "            landmarks_3d_pose = pose_flat.reshape((POSE_LANDMARKS_COUNT, 3))\n",
        "\n",
        "            # Check for empty face\n",
        "            if np.all(landmarks_3d_face == 0):\n",
        "                engineered_feature_sequence.append(np.zeros(NUM_ENGINEERED_FEATURES))\n",
        "                continue\n",
        "\n",
        "            landmarks_2d_face = get_2d_points(landmarks_3d_face, VIDEO_WIDTH, VIDEO_HEIGHT)\n",
        "\n",
        "            # 1. & 2. Eye Aspect Ratios\n",
        "            left_ear = get_ear(landmarks_2d_face[LEFT_EYE_LANDMARKS])\n",
        "            right_ear = get_ear(landmarks_2d_face[RIGHT_EYE_LANDMARKS])\n",
        "\n",
        "            # 3, 4, & 5. Head Pose\n",
        "            yaw, pitch, roll = get_head_pose(landmarks_3d_face, VIDEO_WIDTH, VIDEO_HEIGHT)\n",
        "\n",
        "            # 6. & 7. Left Iris Position\n",
        "            left_iris_center = landmarks_2d_face[LEFT_IRIS_LANDMARKS].mean(axis=0)\n",
        "            rel_left_iris_x, rel_left_iris_y = get_relative_iris_pos(\n",
        "                landmarks_2d_face[LEFT_EYE_LANDMARKS], left_iris_center\n",
        "            )\n",
        "\n",
        "            # 8. & 9. Right Iris Position\n",
        "            right_iris_center = landmarks_2d_face[RIGHT_IRIS_LANDMARKS].mean(axis=0)\n",
        "            rel_right_iris_x, rel_right_iris_y = get_relative_iris_pos(\n",
        "                landmarks_2d_face[RIGHT_EYE_LANDMARKS], right_iris_center\n",
        "            )\n",
        "\n",
        "            # 10. Mouth Aspect Ratio\n",
        "            mar = get_mar(landmarks_2d_face[MOUTH_LANDMARKS])\n",
        "\n",
        "            # --- NEW HAND/POSE FEATURES ---\n",
        "            # 11. & 12. Left Wrist Position (x, y)\n",
        "            # Using pose landmarks, which are more stable for wrists\n",
        "            l_wrist_pos = landmarks_3d_pose[LEFT_WRIST_POSE]\n",
        "\n",
        "            # 13. & 14. Right Wrist Position (x, y)\n",
        "            r_wrist_pos = landmarks_3d_pose[RIGHT_WRIST_POSE]\n",
        "\n",
        "            # 15. & 16. Hand-to-Face Distance\n",
        "            # Get 3D position of nose tip\n",
        "            nose_pos = landmarks_3d_face[NOSE_TIP_LANDMARK]\n",
        "\n",
        "            # Calculate 3D Euclidean distance\n",
        "            # Use 0 as a placeholder if hands aren't detected (visibility/presence < 0.5)\n",
        "            # Note: Pose landmarks also have a visibility score, which we are ignoring here\n",
        "            # for simplicity, but a real implementation should check it.\n",
        "\n",
        "            l_hand_dist = get_dist(l_wrist_pos, nose_pos) if np.any(l_wrist_pos != 0) else 0.0\n",
        "            r_hand_dist = get_dist(r_wrist_pos, nose_pos) if np.any(r_wrist_pos != 0) else 0.0\n",
        "\n",
        "            # Append all 16 features\n",
        "            frame_features = [\n",
        "                left_ear, right_ear,\n",
        "                yaw, pitch, roll,\n",
        "                rel_left_iris_x, rel_left_iris_y,\n",
        "                rel_right_iris_x, rel_right_iris_y,\n",
        "                mar,\n",
        "                l_wrist_pos[0], l_wrist_pos[1], # l_wrist_x, l_wrist_y\n",
        "                r_wrist_pos[0], r_wrist_pos[1], # r_wrist_x, r_wrist_y\n",
        "                l_hand_dist, r_hand_dist\n",
        "            ]\n",
        "            engineered_feature_sequence.append(frame_features)\n",
        "\n",
        "        # Save the new, lightweight (num_frames, 16) array\n",
        "        engineered_array = np.array(engineered_feature_sequence, dtype=np.float32)\n",
        "\n",
        "        base_name = os.path.splitext(os.path.basename(file_path))[0]\n",
        "        output_path = os.path.join(ENGINEERED_FEATURES_DIR, f\"{base_name}.npy\")\n",
        "        np.save(output_path, engineered_array)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to process {file_path}. Error: {e}\")\n",
        "\n",
        "print(f\"\\nFeature engineering complete. All lightweight .npy files saved to {ENGINEERED_FEATURES_DIR}\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-25T17:44:54.894338Z",
          "iopub.status.idle": "2025-10-25T17:44:54.894609Z",
          "shell.execute_reply.started": "2025-10-25T17:44:54.894493Z",
          "shell.execute_reply": "2025-10-25T17:44:54.894506Z"
        },
        "id": "wIICRjZoB3FW"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# --- Configuration ---\n",
        "INPUT_CSV = '/kaggle/input/labeled-ocat/grouped_output.csv'  # The CSV file from Step 2\n",
        "OUTPUT_FILE = '/kaggle/working/seed_batch_for_labeling.txt' # File listing the clips to label\n",
        "NUM_CLIPS_PER_CATEGORY = 200\n",
        "\n",
        "# --- Load the Data ---\n",
        "try:\n",
        "    df = pd.read_csv(INPUT_CSV)\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: Could not find the file {INPUT_CSV}\")\n",
        "    print(\"Please make sure you have run the pre-labeling script first.\")\n",
        "    exit()\n",
        "\n",
        "# --- 1. Find Confident 'PRE_Distracted' Clips ---\n",
        "\n",
        "# Filter for the clips labeled as 'PRE_Distracted'\n",
        "df_distracted = df[df['draft_label'] == 'PRE_Distracted'].copy()\n",
        "\n",
        "# Create a \"confidence score\"\n",
        "# This score measures *how far* the values are past the rule thresholds\n",
        "# Rule: (abs(yaw) > 25) OR (eyes_closed > 40)\n",
        "# We use abs() for yaw to handle both left and right turns\n",
        "df_distracted['yaw_score'] = (df_distracted['avg_head_yaw'].abs() - 25).clip(lower=0)\n",
        "df_distracted['blink_score'] = (df_distracted['percent_eyes_closed'] - 40).clip(lower=0)\n",
        "# The final confidence is the max of these two scores\n",
        "df_distracted['confidence'] = df_distracted[['yaw_score', 'blink_score']].max(axis=1)\n",
        "\n",
        "# Sort by the highest confidence and get the top N\n",
        "top_distracted = df_distracted.sort_values(by='confidence', ascending=False).head(NUM_CLIPS_PER_CATEGORY)\n",
        "\n",
        "\n",
        "# --- 2. Find Confident 'PRE_Attentive' Clips ---\n",
        "\n",
        "# Filter for the clips labeled as 'PRE_Attentive'\n",
        "df_attentive = df[df['draft_label'] == 'PRE_Attentive'].copy()\n",
        "\n",
        "# Create a \"confidence score\"\n",
        "# This score measures *how far* the values are *within* the rule thresholds\n",
        "# Rule: (abs(yaw) < 10) AND (gaze_time < 20)\n",
        "df_attentive['yaw_score'] = (10 - df_attentive['avg_head_yaw'].abs()).clip(lower=0)\n",
        "df_attentive['gaze_score'] = (20 - df_attentive['gaze_off_center_time']).clip(lower=0)\n",
        "# The final confidence is the *sum* of these scores (must be good at both)\n",
        "df_attentive['confidence'] = df_attentive['yaw_score'] + df_attentive['gaze_score']\n",
        "\n",
        "# Sort by the highest confidence and get the top N\n",
        "top_attentive = df_attentive.sort_values(by='confidence', ascending=False).head(NUM_CLIPS_PER_CATEGORY)\n",
        "\n",
        "\n",
        "# --- 3. Combine and Save the Seed Batch ---\n",
        "\n",
        "# Combine the clip names from both dataframes\n",
        "seed_batch_clips = pd.concat([top_distracted['clip_name'], top_attentive['clip_name']])\n",
        "\n",
        "# Save the list of clip names to a text file\n",
        "# This is the \"work list\" for your annotators\n",
        "try:\n",
        "    with open(OUTPUT_FILE, 'w') as f:\n",
        "        for clip_name in seed_batch_clips:\n",
        "            f.write(f\"{clip_name}\\n\")\n",
        "\n",
        "    print(f\"Successfully selected seed batch:\")\n",
        "    print(f\"  {len(top_distracted)} 'PRE_Distracted' clips\")\n",
        "    print(f\"  {len(top_attentive)} 'PRE_Attentive' clips\")\n",
        "    print(f\"  ---------------------------------\")\n",
        "    print(f\"  Total: {len(seed_batch_clips)} clips\")\n",
        "    print(f\"\\nThis list has been saved to: {OUTPUT_FILE}\")\n",
        "    print(\"Your annotators should now correct the labels for these clips.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while saving the file: {e}\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-25T17:44:54.895604Z",
          "iopub.status.idle": "2025-10-25T17:44:54.895883Z",
          "shell.execute_reply.started": "2025-10-25T17:44:54.895742Z",
          "shell.execute_reply": "2025-10-25T17:44:54.89576Z"
        },
        "id": "w91yUJyXB3FX"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "# List of source folders\n",
        "folders = [\n",
        "    \"/kaggle/input/labeled-ocat/engineered_features_zip\",\n",
        "    \"/kaggle/input/labeled-ocat/engineered_features_zip1\",\n",
        "    \"/kaggle/input/labeled-ocat/engineered_features_zip2\",\n",
        "    \"/kaggle/input/labeled-ocat/engineered_features_zip3\",\n",
        "    \"/kaggle/input/labeled-ocat/engineered_features_zip4\"\n",
        "]\n",
        "\n",
        "# Destination folder\n",
        "destination = \"path/to/engineered_features_merged\"\n",
        "\n",
        "# Create destination folder if it doesn’t exist\n",
        "os.makedirs(destination, exist_ok=True)\n",
        "\n",
        "# Copy all files from each folder to destination\n",
        "for folder in folders:\n",
        "    for file_name in os.listdir(folder):\n",
        "        src_path = os.path.join(folder, file_name)\n",
        "        dst_path = os.path.join(destination, file_name)\n",
        "\n",
        "        # Avoid overwriting by renaming duplicates\n",
        "        if os.path.exists(dst_path):\n",
        "            name, ext = os.path.splitext(file_name)\n",
        "            counter = 1\n",
        "            while os.path.exists(dst_path):\n",
        "                dst_path = os.path.join(destination, f\"{name}_{counter}{ext}\")\n",
        "                counter += 1\n",
        "\n",
        "        shutil.copy2(src_path, dst_path)\n",
        "\n",
        "print(\"✅ All files merged into:\", destination)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-25T17:44:54.968186Z",
          "iopub.execute_input": "2025-10-25T17:44:54.968423Z",
          "iopub.status.idle": "2025-10-25T17:46:02.82448Z",
          "shell.execute_reply.started": "2025-10-25T17:44:54.9684Z",
          "shell.execute_reply": "2025-10-25T17:46:02.823801Z"
        },
        "id": "Hn2EHMRxB3FX"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read your CSV\n",
        "df = pd.read_csv(\"/kaggle/input/labeled-ocat/grouped_output.csv\")\n",
        "\n",
        "# Replace old values with new ones\n",
        "df[\"draft_label\"] = df[\"draft_label\"].replace({\n",
        "    \"PRE_Attentive\": \"Attentive\",\n",
        "    \"PRE_Neutral\": \"Neutral\",\n",
        "    \"PRE_Distracted\": \"Distracted\"\n",
        "})\n",
        "\n",
        "# Save updated CSV\n",
        "df.to_csv(\"/kaggle/working/updated_output.csv\", index=False)\n",
        "\n",
        "print(\"✅ Values in 'draft_label' column updated and saved as 'updated_output.csv'\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-25T17:46:02.825643Z",
          "iopub.execute_input": "2025-10-25T17:46:02.825858Z",
          "iopub.status.idle": "2025-10-25T17:46:02.888985Z",
          "shell.execute_reply.started": "2025-10-25T17:46:02.825839Z",
          "shell.execute_reply": "2025-10-25T17:46:02.88816Z"
        },
        "id": "EquSa0nVB3FY"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Input\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# --- Configuration ---\n",
        "LABELS_FILE = '/kaggle/input/prelabes/updated_output.csv'         # Your human-verified labels\n",
        "ENGINEERED_FEATURES_DIR = '/kaggle/input/engineered-features' # The folder with (90, 10) .npy files\n",
        "\n",
        "# Model Hyperparameters\n",
        "NUM_EPOCHS = 100\n",
        "BATCH_SIZE = 32\n",
        "MODEL_SAVE_PATH = 'model_v0.1.h5'\n",
        "\n",
        "# --- 1. Load Data ---\n",
        "print(\"Loading data...\")\n",
        "try:\n",
        "    df = pd.read_csv(LABELS_FILE)\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: Could not find the file {LABELS_FILE}\")\n",
        "    print(\"Please make sure you have the 'corrected_labels.csv' from the labeling tool.\")\n",
        "    exit()\n",
        "\n",
        "X = []\n",
        "y = []\n",
        "\n",
        "# Define the expected order of labels for consistency\n",
        "label_mapping = {'Attentive': 0, 'Neutral': 1, 'Distracted': 2}\n",
        "num_classes = len(label_mapping)\n",
        "class_names = [name for name, _ in sorted(label_mapping.items(), key=lambda item: item[1])]\n",
        "\n",
        "for index, row in df.iterrows():\n",
        "    clip_name = row['clip_name']\n",
        "    label = row['draft_label']\n",
        "\n",
        "    # Construct the full path to the feature file\n",
        "    feature_file_path = os.path.join(ENGINEERED_FEATURES_DIR, f\"{clip_name}.npy\")\n",
        "\n",
        "    if os.path.exists(feature_file_path):\n",
        "        # Load the (90, 10) feature array\n",
        "        features = np.load(feature_file_path)\n",
        "        X.append(features)\n",
        "        y.append(label)\n",
        "    else:\n",
        "        print(f\"Warning: Could not find feature file for clip: {clip_name}. Skipping.\")\n",
        "\n",
        "if not X:\n",
        "    print(\"Error: No feature files were loaded. Please check the ENGINEERED_FEATURES_DIR path.\")\n",
        "    exit()\n",
        "\n",
        "# Pad/truncate sequences to ensure they all have the same length\n",
        "# This is necessary because video clips might have slightly different frame counts (e.g., 89, 90, 91)\n",
        "# We will fix the length to 90 frames (for a 3-second, 30fps clip)\n",
        "FIXED_SEQUENCE_LENGTH = 90\n",
        "X = pad_sequences(\n",
        "    X, maxlen=FIXED_SEQUENCE_LENGTH, dtype='float32', padding='post', truncating='post'\n",
        ")\n",
        "\n",
        "# Convert lists to numpy arrays\n",
        "# The line below is no longer needed as pad_sequences returns a numpy array\n",
        "# X = np.array(X)\n",
        "y = np.array(y)\n",
        "\n",
        "# --- 2. Preprocess Data ---\n",
        "print(\"Preprocessing data...\")\n",
        "# Encode string labels to integers\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(class_names) # Fit on all possible class names to ensure order\n",
        "y_encoded = encoder.transform(y)\n",
        "\n",
        "# Convert integers to one-hot vectors\n",
        "y_one_hot = to_categorical(y_encoded, num_classes=num_classes)\n",
        "\n",
        "# Split data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X, y_one_hot, test_size=0.2, random_state=42, stratify=y_one_hot\n",
        ")\n",
        "\n",
        "# Get input shape from the training data\n",
        "# Should be (num_timesteps, num_features), e.g., (90, 10)\n",
        "input_shape = (X_train.shape[1], X_train.shape[2])\n",
        "\n",
        "print(f\"Data shapes:\")\n",
        "print(f\"  X_train: {X_train.shape}\")\n",
        "print(f\"  y_train: {y_train.shape}\")\n",
        "print(f\"  X_val:   {X_val.shape}\")\n",
        "print(f\"  y_val:   {y_val.shape}\")\n",
        "\n",
        "# --- 3. Define the LSTM Model ---\n",
        "print(\"Building the LSTM model...\")\n",
        "model = Sequential([\n",
        "    Input(shape=input_shape),\n",
        "    LSTM(64, return_sequences=True),\n",
        "    Dropout(0.5),\n",
        "    LSTM(32),\n",
        "    Dropout(0.5),\n",
        "    Dense(16, activation='relu'),\n",
        "    Dense(num_classes, activation='softmax') # Softmax for multi-class classification\n",
        "])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# --- 4. Compile and Train the Model ---\n",
        "print(\"Compiling and training the model...\")\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='categorical_crossentropy', # Use for one-hot encoded labels\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Callbacks for better training\n",
        "# Stop training if validation loss doesn't improve for 10 epochs\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
        "# Save the best model found during training\n",
        "model_checkpoint = ModelCheckpoint(MODEL_SAVE_PATH, monitor='val_accuracy', save_best_only=True)\n",
        "\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=NUM_EPOCHS,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    validation_data=(X_val, y_val),\n",
        "    callbacks=[early_stopping, model_checkpoint]\n",
        ")\n",
        "\n",
        "# --- 5. Final Evaluation ---\n",
        "print(\"\\nTraining complete.\")\n",
        "loss, accuracy = model.evaluate(X_val, y_val, verbose=0)\n",
        "print(f\"Best validation accuracy: {accuracy*100:.2f}%\")\n",
        "print(f\"Model saved to {MODEL_SAVE_PATH}\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-25T17:46:02.889885Z",
          "iopub.execute_input": "2025-10-25T17:46:02.890079Z",
          "iopub.status.idle": "2025-10-25T17:49:26.037022Z",
          "shell.execute_reply.started": "2025-10-25T17:46:02.890063Z",
          "shell.execute_reply": "2025-10-25T17:49:26.036351Z"
        },
        "id": "wJ8TvpaYB3FY"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Input\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras import regularizers\n",
        "\n",
        "# --- Configuration ---\n",
        "LABELS_FILE = '/kaggle/working/updated_output2.csv'       # Your human-verified labels\n",
        "ENGINEERED_FEATURES_DIR = '/kaggle/input/engineered-features' # The folder with (90, 10) .npy files\n",
        "FIXED_SEQUENCE_LENGTH = 90\n",
        "NEXT_BATCH_SIZE = 300 # Number of clips to select for next round\n",
        "NEXT_BATCH_FILE = '/kaggle/working/seed_labels2.csv'\n",
        "\n",
        "# Model Hyperparameters\n",
        "NUM_EPOCHS = 100\n",
        "BATCH_SIZE = 32\n",
        "MODEL_SAVE_PATH = '/kaggle/working/model_v0.1.h5' # Save to working dir\n",
        "\n",
        "# Define the expected order of labels for consistency\n",
        "label_mapping = {'Attentive': 0, 'Neutral': 1, 'Distracted': 2}\n",
        "num_classes = len(label_mapping)\n",
        "class_names = [name for name, _ in sorted(label_mapping.items(), key=lambda item: item[1])]\n",
        "\n",
        "# --- 1. Load Data ---\n",
        "print(\"Loading data...\")\n",
        "try:\n",
        "    df = pd.read_csv(LABELS_FILE)\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: Could not find the file {LABELS_FILE}\")\n",
        "    print(\"Please make sure you have the 'corrected_labels.csv' from the labeling tool.\")\n",
        "    exit()\n",
        "\n",
        "X = []\n",
        "y = []\n",
        "\n",
        "for index, row in df.iterrows():\n",
        "    clip_name = row['clip_name']\n",
        "    label = row['draft_label']\n",
        "\n",
        "    # Construct the full path to the feature file\n",
        "    feature_file_path = os.path.join(ENGINEERED_FEATURES_DIR, f\"{clip_name}.npy\")\n",
        "\n",
        "    if os.path.exists(feature_file_path):\n",
        "        # Load the (90, 10) feature array\n",
        "        features = np.load(feature_file_path)\n",
        "        X.append(features)\n",
        "        y.append(label)\n",
        "    else:\n",
        "        print(f\"Warning: Could not find feature file for clip: {clip_name}. Skipping.\")\n",
        "\n",
        "if not X:\n",
        "    print(\"Error: No feature files were loaded. Please check the ENGINEERED_FEATURES_DIR path.\")\n",
        "    exit()\n",
        "\n",
        "X = pad_sequences(\n",
        "    X, maxlen=FIXED_SEQUENCE_LENGTH, dtype='float32', padding='post', truncating='post'\n",
        ")\n",
        "y = np.array(y)\n",
        "\n",
        "# --- 2. Preprocess Data ---\n",
        "print(\"Preprocessing data...\")\n",
        "# Encode string labels to integers\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(class_names) # Fit on all possible class names to ensure order\n",
        "y_encoded = encoder.transform(y)\n",
        "\n",
        "# Convert integers to one-hot vectors\n",
        "y_one_hot = to_categorical(y_encoded, num_classes=num_classes)\n",
        "\n",
        "# Split data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X, y_one_hot, test_size=0.2, random_state=42, stratify=y_one_hot\n",
        ")\n",
        "\n",
        "# Get input shape from the training data\n",
        "input_shape = (X_train.shape[1], X_train.shape[2])\n",
        "\n",
        "print(f\"Data shapes:\")\n",
        "print(f\"  X_train: {X_train.shape}\")\n",
        "print(f\"  y_train: {y_train.shape}\")\n",
        "print(f\"  X_val:   {X_val.shape}\")\n",
        "print(f\"  y_val:   {y_val.shape}\")\n",
        "\n",
        "# --- 3. Define the LSTM Model ---\n",
        "print(\"Building the LSTM model...\")\n",
        "model = Sequential([\n",
        "    Input(shape=input_shape),\n",
        "    LSTM(64, return_sequences=True),\n",
        "    Dropout(0.5),\n",
        "    LSTM(32),\n",
        "    Dropout(0.5),\n",
        "    Dense(16, activation='relu'),\n",
        "    Dense(num_classes, activation='softmax') # Softmax for multi-class classification\n",
        "])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# --- 4. Compile and Train the Model ---\n",
        "print(\"Compiling and training the model...\")\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='categorical_crossentropy', # Use for one-hot encoded labels\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Callbacks for better training\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
        "model_checkpoint = ModelCheckpoint(MODEL_SAVE_PATH, monitor='val_accuracy', save_best_only=True)\n",
        "\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=NUM_EPOCHS,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    validation_data=(X_val, y_val),\n",
        "    callbacks=[early_stopping, model_checkpoint]\n",
        ")\n",
        "\n",
        "# --- 5. Final Evaluation ---\n",
        "print(\"\\nTraining complete.\")\n",
        "loss, accuracy = model.evaluate(X_val, y_val, verbose=0)\n",
        "print(f\"Best validation accuracy: {accuracy*100:.2f}%\")\n",
        "print(f\"Model saved to {MODEL_SAVE_PATH}\")\n",
        "\n",
        "# --- 6. Active Learning: Find Next Batch ---\n",
        "print(\"\\n--- Starting Active Learning ---\")\n",
        "print(f\"Loading best model from {MODEL_SAVE_PATH} to find disagreements...\")\n",
        "\n",
        "# Load the best model that was just saved\n",
        "try:\n",
        "    model = load_model(MODEL_SAVE_PATH)\n",
        "except Exception as e:\n",
        "    print(f\"Error loading model: {e}\")\n",
        "    exit()\n",
        "\n",
        "# We need to predict on ALL data, not just the training split.\n",
        "# So we re-load all feature data from the original dataframe.\n",
        "X_all = []\n",
        "y_draft_labels_all = []\n",
        "all_clip_names = []\n",
        "\n",
        "print(f\"Loading all {len(df)} clips from {LABELS_FILE} for prediction...\")\n",
        "for index, row in df.iterrows():\n",
        "    clip_name = row['clip_name']\n",
        "    feature_file_path = os.path.join(ENGINEERED_FEATURES_DIR, f\"{clip_name}.npy\")\n",
        "\n",
        "    if os.path.exists(feature_file_path):\n",
        "        features = np.load(feature_file_path)\n",
        "        X_all.append(features)\n",
        "        y_draft_labels_all.append(row['draft_label'])\n",
        "        all_clip_names.append(row['clip_name'])\n",
        "    # No need to warn again, we did that in section 1\n",
        "\n",
        "if not X_all:\n",
        "    print(\"Error: No data loaded for prediction.\")\n",
        "    exit()\n",
        "\n",
        "# Pad all sequences identically to how training data was padded\n",
        "X_all = pad_sequences(\n",
        "    X_all, maxlen=FIXED_SEQUENCE_LENGTH, dtype='float32', padding='post', truncating='post'\n",
        ")\n",
        "\n",
        "# Make predictions on all data\n",
        "print(f\"Making predictions on {len(X_all)} clips...\")\n",
        "y_pred_probs = model.predict(X_all) # (N_clips, 3) array of probabilities\n",
        "y_pred_class_indices = np.argmax(y_pred_probs, axis=1) # (N_clips,) array of class indices (0, 1, or 2)\n",
        "y_pred_confidence = np.max(y_pred_probs, axis=1) # Confidence for the predicted class\n",
        "\n",
        "# Encode the original draft labels to compare\n",
        "y_draft_encoded = encoder.transform(y_draft_labels_all)\n",
        "\n",
        "# Calculate uncertainty (Shannon Entropy)\n",
        "# H = -sum(p_i * log2(p_i))\n",
        "# High entropy = high uncertainty (e.g., [0.33, 0.33, 0.33])\n",
        "# Low entropy = high certainty (e.g., [0.98, 0.01, 0.01])\n",
        "# We add a small epsilon (1e-9) to avoid log(0)\n",
        "uncertainty = -np.sum(y_pred_probs * np.log2(y_pred_probs + 1e-9), axis=1)\n",
        "\n",
        "# Identify disagreements (Criterion 1)\n",
        "is_disagreement = (y_pred_class_indices != y_draft_encoded)\n",
        "# Score: 0 if agreement, confidence if disagreement\n",
        "disagreement_score = is_disagreement * y_pred_confidence\n",
        "\n",
        "# Create a results DataFrame\n",
        "results_df = pd.DataFrame({\n",
        "    'clip_name': all_clip_names,\n",
        "    'draft_label': y_draft_labels_all,\n",
        "    'predicted_label': encoder.inverse_transform(y_pred_class_indices),\n",
        "    'confidence': y_pred_confidence,\n",
        "    'uncertainty_entropy': uncertainty,\n",
        "    'is_disagreement': is_disagreement,\n",
        "    'disagreement_score': disagreement_score\n",
        "})\n",
        "\n",
        "# Combine scores: We want clips that are high in uncertainty OR high in disagreement score\n",
        "# Adding them gives a good composite score for ranking\n",
        "results_df['final_score'] = results_df['uncertainty_entropy'] + results_df['disagreement_score']\n",
        "\n",
        "# Sort by the final score (highest first)\n",
        "results_df = results_df.sort_values(by='final_score', ascending=False)\n",
        "\n",
        "# Select the top N clips for the next batch\n",
        "next_batch_df = results_df.head(NEXT_BATCH_SIZE)\n",
        "\n",
        "# Save the batch for the labeling tool (clip_name and its original draft_label)\n",
        "output_df = next_batch_df[['clip_name', 'draft_label']]\n",
        "output_df.to_csv(NEXT_BATCH_FILE, index=False)\n",
        "\n",
        "print(f\"\\nActive learning batch created!\")\n",
        "print(f\"Top {NEXT_BATCH_SIZE} clips saved to: {NEXT_BATCH_FILE}\")\n",
        "print(\"\\n--- Top 5 most valuable clips for relabeling ---\")\n",
        "print(results_df.head(5))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-25T17:49:26.038605Z",
          "iopub.execute_input": "2025-10-25T17:49:26.038845Z"
        },
        "id": "B6Rpj6yvB3FY"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "BYyM2WiuB3FZ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import ipywidgets as widgets\n",
        "from IPython.display import display, HTML, clear_output\n",
        "from base64 import b64encode\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# --- 1. Display Function ---\n",
        "def display_video(path):\n",
        "    try:\n",
        "        mp4 = open(path, \"rb\").read()\n",
        "        data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "        return HTML(f\"\"\"\n",
        "        <video width=400 controls>\n",
        "              <source src=\"{data_url}\" type=\"video/mp4\">\n",
        "        </video>\n",
        "        \"\"\")\n",
        "    except Exception as e:\n",
        "        return HTML(f\"<p>Error loading video: {e}</p>\")\n",
        "\n",
        "# --- 2. Load Clip List from seed_labels1.csv ---\n",
        "\n",
        "# --- MODIFIED: Load directly from seed_labels1.csv ---\n",
        "labels_csv_path = \"/kaggle/working/seed_labels1.csv\" # <-- Using the new file\n",
        "\n",
        "clip_data = [] # Will store tuples (clip_name, draft_label)\n",
        "try:\n",
        "    # Read the new CSV file\n",
        "    labels_df = pd.read_csv(labels_csv_path)\n",
        "\n",
        "    # --- ASSUMPTION ---\n",
        "    # We assume your CSV has a 'clip_name' column for the filename\n",
        "    # and a 'draft_label' column for the draft label.\n",
        "    clip_data = list(zip(labels_df['clip_name'], labels_df['draft_label']))\n",
        "\n",
        "    print(f\"Loaded {len(clip_data)} clips to label from {labels_csv_path}\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: Could not find file {labels_csv_path}.\")\n",
        "    print(\"Please make sure 'seed_labels1.csv' is in that location.\")\n",
        "    clip_data = [(\"dummy.mp4\", \"Unknown\")] # Add dummy to prevent crash\n",
        "except Exception as e:\n",
        "    print(f\"Error reading main labels file {labels_csv_path}: {e}\")\n",
        "    if not clip_data:\n",
        "        clip_data = [(\"dummy.mp4\", \"Unknown\")]\n",
        "# --- End of Modification ---\n",
        "\n",
        "\n",
        "# Two main folders where clips may be located\n",
        "clip_dirs = [\n",
        "    \"/kaggle/input/ocat-clips/ocat clips1/ocat clips1\",\n",
        "    \"/kaggle/input/ocat-clips/ocat clips2/ocat clips2\"\n",
        "]\n",
        "\n",
        "# Locate each video file and build the list for the DataFrame\n",
        "video_paths = []\n",
        "draft_labels = []\n",
        "# --- MODIFICATION: Add clip_name to the dataframe ---\n",
        "clip_names_list = []\n",
        "# --- End Modification ---\n",
        "\n",
        "for clip, label in clip_data:\n",
        "    # Ensure filename ends with .mp4\n",
        "    if not clip.endswith(\".mp4\"):\n",
        "        clip += \".mp4\"\n",
        "    found = False\n",
        "    for folder in clip_dirs:\n",
        "        path = os.path.join(folder, clip)\n",
        "        if os.path.exists(path):\n",
        "            video_paths.append(path)\n",
        "            draft_labels.append(label) # Add the corresponding label\n",
        "            clip_names_list.append(clip) # --- Add clip name\n",
        "            found = True\n",
        "            break\n",
        "    if not found and clip != \"dummy.mp4\": # Don't warn for the dummy clip\n",
        "        print(f\"⚠️ Missing: {clip}\")\n",
        "\n",
        "# Create annotation DataFrame\n",
        "annotation_df = pd.DataFrame({\n",
        "    \"clip_path\": video_paths,\n",
        "    \"clip_name\": clip_names_list, # --- Add clip name column\n",
        "    \"draft_label\": draft_labels, # Use the loaded labels\n",
        "    \"final_label\": [None] * len(video_paths)\n",
        "})\n",
        "\n",
        "LABEL_CHOICES = ['Attentive', 'Neutral', 'Distracted']\n",
        "\n",
        "# --- 3. Create Widgets ---\n",
        "video_output = widgets.Output()\n",
        "draft_label_display = widgets.HTML(value=\"<h3>Draft Label: -</h3>\")\n",
        "label_dropdown = widgets.Dropdown(\n",
        "    options=LABEL_CHOICES,\n",
        "    description='Correct Label:',\n",
        "    style={'description_width': 'initial'}\n",
        ")\n",
        "submit_button = widgets.Button(\n",
        "    description='Confirm & Next',\n",
        "    button_style='success',\n",
        "    icon='check'\n",
        ")\n",
        "progress_label = widgets.Label(value=\"Clip 0 of 0\")\n",
        "\n",
        "# --- 4. Logic for Navigation ---\n",
        "class AnnotatorState:\n",
        "    index = 0\n",
        "\n",
        "state = AnnotatorState()\n",
        "\n",
        "def load_clip(idx):\n",
        "    if idx >= len(annotation_df):\n",
        "        with video_output:\n",
        "            clear_output()\n",
        "            print(\"Annotation complete! 🎉\")\n",
        "        draft_label_display.value = \"<h3>All Done!</h3>\"\n",
        "        label_dropdown.disabled = True\n",
        "        submit_button.disabled = True\n",
        "        progress_label.value = \"Finished.\"\n",
        "        return\n",
        "\n",
        "    row = annotation_df.iloc[idx]\n",
        "    progress_label.value = f\"Clip {idx + 1} of {len(annotation_df)}\"\n",
        "\n",
        "    current_draft_label = row['draft_label']\n",
        "    draft_label_display.value = f\"<h3>Draft Label: {current_draft_label}</h3>\"\n",
        "\n",
        "    # Set dropdown default to draft label if valid\n",
        "    if current_draft_label in LABEL_CHOICES:\n",
        "        label_dropdown.value = current_draft_label\n",
        "    else:\n",
        "        label_dropdown.value = LABEL_CHOICES[0] # Default to first item\n",
        "\n",
        "    with video_output:\n",
        "        clear_output(wait=True)\n",
        "        if os.path.exists(row['clip_path']):\n",
        "            display(display_video(row['clip_path']))\n",
        "        else:\n",
        "            display(HTML(f\"<p><b>Error:</b> File not found at {row['clip_path']}</p>\"))\n",
        "\n",
        "def on_submit_click(b):\n",
        "    if state.index < len(annotation_df):\n",
        "        annotation_df.loc[state.index, 'final_label'] = label_dropdown.value\n",
        "    state.index += 1\n",
        "    load_clip(state.index)\n",
        "\n",
        "submit_button.on_click(on_submit_click)\n",
        "\n",
        "# --- 5. Display Tool ---\n",
        "ui = widgets.VBox([\n",
        "    progress_label,\n",
        "    video_output,\n",
        "    draft_label_display,\n",
        "    label_dropdown,\n",
        "    submit_button\n",
        "])\n",
        "\n",
        "# --- MODIFIED: Updated print message ---\n",
        "print(f\"Loaded {len(annotation_df)} valid videos to label from {labels_csv_path}\")\n",
        "\n",
        "if len(annotation_df) > 0:\n",
        "    load_clip(state.index)\n",
        "    display(ui)\n",
        "else:\n",
        "    print(\"No videos found to label.\")\n",
        "\n",
        "# You can access your results after labeling by checking the DataFrame:\n",
        "# print(annotation_df)"
      ],
      "metadata": {
        "trusted": true,
        "id": "fTju7dyTB3FZ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "annotation_df"
      ],
      "metadata": {
        "trusted": true,
        "id": "WA2KvzYFB3Fa"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "annotation_df['clip_name'] = annotation_df['clip_path'].str.replace(\n",
        "    r'^/kaggle/input/ocat-clips/ocat clips[12]/ocat clips[12]/', '', regex=True\n",
        ")\n",
        "\n",
        "# Remove '.mp4' extension from clip_name\n",
        "annotation_df['clip_name'] = annotation_df['clip_name'].str.replace('.mp4', '', regex=False)\n",
        "\n",
        "annotation_df=annotation_df.drop('clip_path',axis=1)\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "VF_qZMX3B3Fb"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "annotation_df"
      ],
      "metadata": {
        "trusted": true,
        "id": "J5MguJ_NB3Fb"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# --- Configuration ---\n",
        "original_csv_path = \"/kaggle/working/updated_output1.csv\"\n",
        "output_csv_path = \"/kaggle/working/updated_output2.csv\"\n",
        "\n",
        "# --- Main Logic ---\n",
        "try:\n",
        "    # 1. Load the original CSV file\n",
        "    original_df = pd.read_csv(original_csv_path)\n",
        "\n",
        "    # 2. Prepare the updates from annotation_df\n",
        "    updates_to_apply = annotation_df[annotation_df['final_label'].notna()].copy()\n",
        "\n",
        "    if updates_to_apply.empty:\n",
        "        print(\"No new labels found in 'annotation_df'. No updates will be made.\")\n",
        "    else:\n",
        "        # 3. Create a mapping dictionary from clip_name -> final_label\n",
        "        # Drop duplicates in case you accidentally relabeled the same clip twice in the UI\n",
        "        label_map = updates_to_apply.drop_duplicates(subset='clip_name').set_index('clip_name')['final_label']\n",
        "\n",
        "        # 4. Update the original DataFrame using .map()\n",
        "        # This is the key change to fix the error and warning.\n",
        "\n",
        "        # Create a series of new labels by mapping from the 'clip_name' column\n",
        "        new_labels = original_df['clip_name'].map(label_map)\n",
        "\n",
        "        # The 'new_labels' series will have NaN for clips that were not relabeled.\n",
        "        # We use .fillna() to keep the original 'draft_label' for those rows.\n",
        "        original_df['draft_label'] = new_labels.fillna(original_df['draft_label'])\n",
        "\n",
        "        # 5. Save the final, updated DataFrame\n",
        "        original_df.to_csv(output_csv_path, index=False)\n",
        "\n",
        "        print(f\"Successfully updated labels for {len(label_map)} unique clip names.\")\n",
        "        print(f\"New file saved to: {output_csv_path}\")\n",
        "\n",
        "except NameError:\n",
        "    print(\"Error: The DataFrame 'annotation_df' was not found.\")\n",
        "    print(\"Please make sure you have run the annotation tool cell first.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: The file {original_csv_path} was not found.\")\n",
        "except KeyError as e:\n",
        "    print(f\"Error: A required column is missing. Details: {e}\")\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred: {e}\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "MvAjPz6zB3Fc"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import ipywidgets as widgets\n",
        "from IPython.display import display, HTML, clear_output\n",
        "from base64 import b64encode\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# --- 1. Display Function ---\n",
        "def display_video(path):\n",
        "    try:\n",
        "        mp4 = open(path, \"rb\").read()\n",
        "        data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "        return HTML(f\"\"\"\n",
        "        <video width=400 controls>\n",
        "              <source src=\"{data_url}\" type=\"video/mp4\">\n",
        "        </video>\n",
        "        \"\"\")\n",
        "    except Exception as e:\n",
        "        return HTML(f\"<p>Error loading video: {e}</p>\")\n",
        "\n",
        "# --- 2. Load Clip List from Text File ---\n",
        "clip_list_path = \"/kaggle/input/prelabes/seed_batch_for_labeling.txt\"\n",
        "\n",
        "# This now assumes the text file is \"clip_name.mp4,draft_label\"\n",
        "clip_data = [] # Will store tuples (clip_name, draft_label)\n",
        "try:\n",
        "    with open(clip_list_path, \"r\") as f:\n",
        "        for line in f:\n",
        "            if line.strip():\n",
        "                parts = line.strip().split(',')\n",
        "                if len(parts) == 2:\n",
        "                    clip_data.append((parts[0].strip(), parts[1].strip()))\n",
        "                elif len(parts) == 1:\n",
        "                     # Fallback if no label is provided\n",
        "                    clip_data.append((parts[0].strip(), \"Unknown\"))\n",
        "except Exception as e:\n",
        "    print(f\"Error reading {clip_list_path}: {e}\")\n",
        "    # Add a dummy entry to prevent crashes later if file is missing\n",
        "    if not clip_data:\n",
        "        clip_data = [(\"dummy.mp4\", \"Unknown\")]\n",
        "\n",
        "\n",
        "# Two main folders where clips may be located\n",
        "clip_dirs = [\n",
        "    \"/kaggle/input/ocat-clips/ocat clips1/ocat clips1\",\n",
        "    \"/kaggle/input/ocat-clips/ocat clips2/ocat clips2\"\n",
        "]\n",
        "\n",
        "# Locate each video file and build the list for the DataFrame\n",
        "video_paths = []\n",
        "draft_labels = []\n",
        "for clip, label in clip_data:\n",
        "    # Ensure filename ends with .mp4\n",
        "    if not clip.endswith(\".mp4\"):\n",
        "        clip += \".mp4\"\n",
        "    found = False\n",
        "    for folder in clip_dirs:\n",
        "        path = os.path.join(folder, clip)\n",
        "        if os.path.exists(path):\n",
        "            video_paths.append(path)\n",
        "            draft_labels.append(label) # Add the corresponding label\n",
        "            found = True\n",
        "            break\n",
        "    if not found and clip != \"dummy.mp4\": # Don't warn for the dummy clip\n",
        "        print(f\"⚠️ Missing: {clip}\")\n",
        "\n",
        "# Create annotation DataFrame\n",
        "annotation_df = pd.DataFrame({\n",
        "    \"clip_path\": video_paths,\n",
        "    \"draft_label\": draft_labels, # Use the loaded labels\n",
        "    \"final_label\": [None] * len(video_paths)\n",
        "})\n",
        "\n",
        "# --- MODIFIED: Updated label choices ---\n",
        "LABEL_CHOICES = ['attention', 'neutral', 'distracted']\n",
        "\n",
        "# --- 3. Create Widgets ---\n",
        "video_output = widgets.Output()\n",
        "draft_label_display = widgets.HTML(value=\"<h3>Draft Label: -</h3>\")\n",
        "label_dropdown = widgets.Dropdown(\n",
        "    options=LABEL_CHOICES,\n",
        "    description='Correct Label:',\n",
        "    style={'description_width': 'initial'}\n",
        ")\n",
        "submit_button = widgets.Button(\n",
        "    description='Confirm & Next',\n",
        "    button_style='success',\n",
        "    icon='check'\n",
        ")\n",
        "progress_label = widgets.Label(value=\"Clip 0 of 0\")\n",
        "\n",
        "# --- 4. Logic for Navigation ---\n",
        "class AnnotatorState:\n",
        "    index = 0\n",
        "\n",
        "state = AnnotatorState()\n",
        "\n",
        "def load_clip(idx):\n",
        "    if idx >= len(annotation_df):\n",
        "        with video_output:\n",
        "            clear_output()\n",
        "            print(\"Annotation complete! 🎉\")\n",
        "        draft_label_display.value = \"<h3>All Done!</h3>\"\n",
        "        label_dropdown.disabled = True\n",
        "        submit_button.disabled = True\n",
        "        progress_label.value = \"Finished.\"\n",
        "        return\n",
        "\n",
        "    row = annotation_df.iloc[idx]\n",
        "    progress_label.value = f\"Clip {idx + 1} of {len(annotation_df)}\"\n",
        "\n",
        "    current_draft_label = row['draft_label']\n",
        "    draft_label_display.value = f\"<h3>Draft Label: {current_draft_label}</h3>\"\n",
        "\n",
        "    # --- MODIFIED: Set dropdown default to draft label if valid ---\n",
        "    if current_draft_label in LABEL_CHOICES:\n",
        "        label_dropdown.value = current_draft_label\n",
        "    else:\n",
        "        label_dropdown.value = LABEL_CHOICES[0] # Default to first item\n",
        "\n",
        "    with video_output:\n",
        "        clear_output(wait=True)\n",
        "        if os.path.exists(row['clip_path']):\n",
        "            display(display_video(row['clip_path']))\n",
        "        else:\n",
        "            display(HTML(f\"<p><b>Error:</b> File not found at {row['clip_path']}</p>\"))\n",
        "\n",
        "def on_submit_click(b):\n",
        "    if state.index < len(annotation_df):\n",
        "        annotation_df.loc[state.index, 'final_label'] = label_dropdown.value\n",
        "    state.index += 1\n",
        "    load_clip(state.index)\n",
        "\n",
        "submit_button.on_click(on_submit_click)\n",
        "\n",
        "# --- 5. Display Tool ---\n",
        "ui = widgets.VBox([\n",
        "    progress_label,\n",
        "    video_output,\n",
        "    draft_label_display,\n",
        "    label_dropdown,\n",
        "    submit_button\n",
        "])\n",
        "\n",
        "print(f\"Loaded {len(annotation_df)} valid videos.\")\n",
        "if len(annotation_df) > 0:\n",
        "    load_clip(state.index)\n",
        "    display(ui)\n",
        "else:\n",
        "    print(\"No videos found to label.\")\n",
        "\n",
        "# You can access your results after labeling by checking the DataFrame:\n",
        "# print(annotation_df)"
      ],
      "metadata": {
        "trusted": true,
        "id": "OoH4ZIY1B3Fc"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "rVYoFhATB3Fd"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Update package list and install the ffmpeg binary\n",
        "!apt-get update && apt-get install -y ffmpeg\n",
        "\n",
        "# Install augly with its video dependencies\n",
        "!pip install augly[video]"
      ],
      "metadata": {
        "trusted": true,
        "id": "FWqFiw1qB3Fd"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, HTML\n",
        "from base64 import b64encode\n",
        "import os\n",
        "\n",
        "# 1. Set the path to your video file\n",
        "video_path = \"/kaggle/input/ocat-clips/ocat clips1/ocat clips1/cropped_2_clip_015.mp4\"\n",
        "\n",
        "# 2. Check if the file exists\n",
        "if os.path.exists(video_path):\n",
        "    # 3. Read the video file in binary mode\n",
        "    mp4 = open(video_path, \"rb\").read()\n",
        "\n",
        "    # 4. Encode the video data to base64\n",
        "    data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "\n",
        "    # 5. Create the HTML string with the embedded video\n",
        "    # The 'controls' attribute adds play/pause, volume, etc.\n",
        "    html_code = f\"\"\"\n",
        "    <video width=400 controls>\n",
        "          <source src=\"{data_url}\" type=\"video/mp4\">\n",
        "          Your browser does not support the video tag.\n",
        "    </video>\n",
        "    \"\"\"\n",
        "\n",
        "    # 6. Display the HTML\n",
        "    display(HTML(html_code))\n",
        "\n",
        "else:\n",
        "    print(f\"Error: Video file not found at {video_path}\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "6DUoGwHWB3Fd"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}